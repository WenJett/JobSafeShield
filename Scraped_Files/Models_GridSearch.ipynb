{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for best params using gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random forest - on the original data\n",
    "rfc_grid = RandomForestClassifier()\n",
    "\n",
    "# Define the grid of parameters to search through\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum number of samples required at each leaf node\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=rfc_grid, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on your data\n",
    "grid_search.fit(xtrain_resampled, ytrain_resampled)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best model from the grid search\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# predictions\n",
    "rfc_grid_ypred = best_rf_model.predict(xtest)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(ytest, rfc_grid_ypred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(ytest, rfc_grid_ypred))\n",
    "\n",
    "# Assuming y_true and y_pred are your true labels and predicted labels\n",
    "conf_matrix = confusion_matrix(ytest, rfc_grid_ypred)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(ytest, rfc_grid_ypred)\n",
    "precision = precision_score(ytest, rfc_grid_ypred)\n",
    "recall = recall_score(ytest, rfc_grid_ypred)\n",
    "f1 = f1_score(ytest, rfc_grid_ypred)\n",
    "\n",
    "# Display the confusion matrix and metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
